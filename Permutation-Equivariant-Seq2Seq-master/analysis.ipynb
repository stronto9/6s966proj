{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.geeksforgeeks.org/python-import-from-sibling-directory/\n",
    "import sys\n",
    " \n",
    "# append the path of the\n",
    "# parent directory\n",
    "sys.path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/gordon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perm_equivariant_seq2seq.equivariant_models import EquiSeq2Seq\n",
    "from perm_equivariant_seq2seq.data_utils import get_scan_split, get_equivariant_scan_languages\n",
    "from perm_equivariant_seq2seq.symmetry_groups import get_permutation_equivariance\n",
    "from perm_equivariant_seq2seq.utils import tensors_from_pair, tensor_from_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 3 is using alternating permutations for 400,000 iterations\n",
    "# model 1 is using circle shifts\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs, test_pairs = get_scan_split(split='add_jump')\n",
    "in_equivariances = ['jump', 'run', 'walk', 'look']\n",
    "out_equivariances = ['JUMP', 'RUN', 'WALK', 'LOOK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equivariant_commands, equivariant_actions = get_equivariant_scan_languages(pairs=train_pairs, input_equivariances=in_equivariances, output_equivariances=out_equivariances)\n",
    "input_symmetry_group = get_permutation_equivariance(equivariant_commands)\n",
    "output_symmetry_group = get_permutation_equivariance(equivariant_actions)\n",
    "\n",
    "hidden_size = 64 # default\n",
    "layer_type = 'GLSTM' # default\n",
    "use_attention = True\n",
    "bidirectional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_state_dict = torch.load(\"models/add_jump/rnn_GLSTM_hidden_64_directions_2/model3/model_fully_trained.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_model = EquiSeq2Seq(input_symmetry_group=input_symmetry_group,\n",
    "                        output_symmetry_group=output_symmetry_group,\n",
    "                        input_language=equivariant_commands,\n",
    "                        encoder_hidden_size=hidden_size,\n",
    "                        decoder_hidden_size=hidden_size,\n",
    "                        output_language=equivariant_actions,\n",
    "                        layer_type=layer_type,\n",
    "                        use_attention=use_attention,\n",
    "                        bidirectional=bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_model.load_state_dict(ap_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_model.to(device)\n",
    "ap_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_state_dict = torch.load(\"models/add_jump/rnn_GLSTM_hidden_64_directions_2/model1/model_fully_trained.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0 # hard-coded in language\n",
    "EOS_token = 1\n",
    "pair = ['jump', 'JUMP']\n",
    "input_t, output_t = tensors_from_pair(pair, equivariant_commands, equivariant_actions)\n",
    "model_sentence = ap_model(input_t)\n",
    "_, sentence_ints = model_sentence.data.topk(1)\n",
    "try:\n",
    "    eos_location = (sentence_ints == EOS_token).nonzero()[0][0]\n",
    "except:\n",
    "    eos_location = len(sentence_ints) - 2\n",
    "model_sentence = sentence_ints[:eos_location+1]\n",
    "print(model_sentence)\n",
    "print(output_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_correct(target, model_sentence):\n",
    "    # First, extract sentence up to EOS\n",
    "    _, sentence_ints = model_sentence.data.topk(1)\n",
    "    # If there is no EOS token, take the complete list\n",
    "    try:\n",
    "        eos_location = (sentence_ints == EOS_token).nonzero()[0][0]\n",
    "    except:\n",
    "        eos_location = len(sentence_ints) - 2\n",
    "    model_sentence = sentence_ints[:eos_location+1]\n",
    "    # Check length is correct\n",
    "    if len(model_sentence) != len(target):\n",
    "        return torch.tensor(0, device=device)\n",
    "    else:\n",
    "        correct = model_sentence == target\n",
    "        return torch.prod(correct).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_correct(output_t, ap_model(input_t))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
